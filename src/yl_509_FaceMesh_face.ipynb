{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "467cfc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765435501.462362   64186 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1765435501.466149   64246 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1765435501.470810   64239 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1765435501.472159   64186 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1765435501.475097   64256 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1765435501.494355   64249 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1765435501.496191   64236 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1765435502.065244   64233 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# 만화 얼굴 PNG 이미지 로드\n",
    "face_img_paths = [\n",
    "    '/home/jm/dev_ws/mldl/src/face_0_default.png',\n",
    "    '/home/jm/dev_ws/mldl/src/face_1_happy.png',\n",
    "    '/home/jm/dev_ws/mldl/src/face_2_surprise.png',\n",
    "]\n",
    "face_imgs = [cv2.imread(p, cv2.IMREAD_UNCHANGED) for p in face_img_paths]\n",
    "# 시청자 기준 왼쪽 기울임용 happy 좌우반전 준비 (cv2.flip(img, 1))\n",
    "happy_flipped = cv2.flip(face_imgs[1], 1) if face_imgs[1] is not None else None\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=6,  # 여러 얼굴 처리\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.3,\n",
    "    min_tracking_confidence=0.3,\n",
    ")\n",
    "\n",
    "mp_face_det = mp.solutions.face_detection\n",
    "face_det = mp_face_det.FaceDetection(model_selection=1, min_detection_confidence=0.3)\n",
    "\n",
    "# 얼굴 각도 계산 (양눈 기준 기울기, 좌우 뒤집힘 보정)\n",
    "def get_face_angle(landmarks, frame_shape):\n",
    "    left_eye = landmarks[33]\n",
    "    right_eye = landmarks[263]\n",
    "    lx, ly = int(left_eye.x * frame_shape[1]), int(left_eye.y * frame_shape[0])\n",
    "    rx, ry = int(right_eye.x * frame_shape[1]), int(right_eye.y * frame_shape[0])\n",
    "    angle = -np.degrees(np.arctan2(ry - ly, rx - lx))  # flip된 화면 보정\n",
    "    return angle\n",
    "\n",
    "# 얼굴 bbox, 중심, 크기 계산\n",
    "def get_face_bbox(landmarks, frame_shape):\n",
    "    points = np.array([(int(l.x * frame_shape[1]), int(l.y * frame_shape[0])) for l in landmarks])\n",
    "    x, y, w, h = cv2.boundingRect(points)\n",
    "    cx, cy = x + w // 2, y + h // 2\n",
    "    return x, y, w, h, cx, cy\n",
    "\n",
    "# FaceDetection 결과에서 bbox/각도 추출 (마스크/안경 fallback)\n",
    "def bbox_angle_from_detection(det, frame_shape):\n",
    "    rel_box = det.location_data.relative_bounding_box\n",
    "    x = int(rel_box.xmin * frame_shape[1])\n",
    "    y = int(rel_box.ymin * frame_shape[0])\n",
    "    w = int(rel_box.width * frame_shape[1])\n",
    "    h = int(rel_box.height * frame_shape[0])\n",
    "    cx, cy = x + w // 2, y + h // 2\n",
    "    if det.location_data.relative_keypoints:\n",
    "        kp = det.location_data.relative_keypoints\n",
    "        lx = int(kp[0].x * frame_shape[1])\n",
    "        ly = int(kp[0].y * frame_shape[0])\n",
    "        rx = int(kp[1].x * frame_shape[1])\n",
    "        ry = int(kp[1].y * frame_shape[0])\n",
    "        angle = -np.degrees(np.arctan2(ry - ly, rx - lx))\n",
    "    else:\n",
    "        angle = 0.0\n",
    "    return (x, y, w, h, cx, cy), angle\n",
    "\n",
    "# 얼굴 마스크 생성 (컨벡스 헐 + 부드러운 feather)\n",
    "def build_skin_mask(landmarks, frame_shape):\n",
    "    pts = np.array([(int(l.x * frame_shape[1]), int(l.y * frame_shape[0])) for l in landmarks], dtype=np.int32)\n",
    "    hull = cv2.convexHull(pts)\n",
    "    mask = np.zeros(frame_shape[:2], dtype=np.uint8)\n",
    "    cv2.fillConvexPoly(mask, hull, 255)\n",
    "    mask = cv2.GaussianBlur(mask, (51, 51), 0)\n",
    "    return mask.astype(np.float32) / 255.0\n",
    "\n",
    "# detection 기반 마스크 (랜드마크 없을 때 타원)\n",
    "def build_ellipse_mask(bbox, frame_shape):\n",
    "    x, y, w, h, cx, cy = bbox\n",
    "    mask = np.zeros(frame_shape[:2], dtype=np.uint8)\n",
    "    axes = (max(1, w // 2), max(1, h // 2))\n",
    "    cv2.ellipse(mask, (cx, cy), axes, 0, 0, 360, 255, -1)\n",
    "    mask = cv2.GaussianBlur(mask, (51, 51), 0)\n",
    "    return mask.astype(np.float32) / 255.0\n",
    "\n",
    "# 살구색으로 얼굴 채우기\n",
    "def blend_skin(frame, mask, skin_color=(190, 220, 255)):\n",
    "    color_img = np.full_like(frame, skin_color)\n",
    "    mask_3d = mask[..., None]\n",
    "    blended = frame * (1 - mask_3d) + color_img * mask_3d\n",
    "    return blended.astype(np.uint8)\n",
    "\n",
    "# 조건별 얼굴 PNG 선택 (최종 분기)\n",
    "# - 가까이 있을 때: surprise\n",
    "# - angle > 18: happy_flipped\n",
    "# - angle < -18: happy 원본\n",
    "# - 기본: default\n",
    "def select_character(bbox, angle, frame_shape):\n",
    "    _, _, w, h, _, _ = bbox\n",
    "    if w > frame_shape[1] * 0.42 or h > frame_shape[0] * 0.42:\n",
    "        return face_imgs[2]\n",
    "    if angle > 18 and happy_flipped is not None:\n",
    "        return happy_flipped\n",
    "    if angle < -18:\n",
    "        return face_imgs[1]\n",
    "    return face_imgs[0]\n",
    "\n",
    "# 얼굴 오버레이 (회전 + 알파) 통합\n",
    "def overlay_character(canvas, bbox, angle, char_img, alpha=0.95, scale=1.12):\n",
    "    if char_img is None:\n",
    "        return canvas\n",
    "\n",
    "    x, y, w, h, cx, cy = bbox\n",
    "    H, W = canvas.shape[:2]\n",
    "    if w <= 0 or h <= 0:\n",
    "        return canvas\n",
    "\n",
    "    target_w = max(1, int(w * scale))\n",
    "    target_h = max(1, int(h * scale))\n",
    "    char_resized = cv2.resize(char_img, (target_w, target_h), interpolation=cv2.INTER_AREA)\n",
    "    M = cv2.getRotationMatrix2D((target_w // 2, target_h // 2), angle, 1.0)\n",
    "    char_rotated = cv2.warpAffine(\n",
    "        char_resized,\n",
    "        M,\n",
    "        (target_w, target_h),\n",
    "        flags=cv2.INTER_LINEAR,\n",
    "        borderMode=cv2.BORDER_CONSTANT,\n",
    "        borderValue=(0, 0, 0, 0),\n",
    "    )\n",
    "\n",
    "    x1 = int(cx - target_w / 2)\n",
    "    y1 = int(cy - target_h / 2)\n",
    "    x2 = x1 + target_w\n",
    "    y2 = y1 + target_h\n",
    "\n",
    "    x1c, y1c = max(0, x1), max(0, y1)\n",
    "    x2c, y2c = min(W, x2), min(H, y2)\n",
    "    if x1c >= x2c or y1c >= y2c:\n",
    "        return canvas\n",
    "\n",
    "    icon_crop = char_rotated[y1c - y1 : y2c - y1, x1c - x1 : x2c - x1]\n",
    "    region = canvas[y1c:y2c, x1c:x2c].astype(np.float32)\n",
    "\n",
    "    if icon_crop.shape[2] == 4:\n",
    "        icon_rgb = icon_crop[:, :, :3].astype(np.float32)\n",
    "        icon_alpha = (icon_crop[:, :, 3] / 255.0 * alpha)[..., None]\n",
    "        blended = region * (1 - icon_alpha) + icon_rgb * icon_alpha\n",
    "        canvas[y1c:y2c, x1c:x2c] = blended.astype(np.uint8)\n",
    "    else:\n",
    "        canvas[y1c:y2c, x1c:x2c] = icon_crop[:, :, :3]\n",
    "    return canvas\n",
    "\n",
    "# 간단한 각도 스무딩 버퍼 (사람별 인덱스 관리)\n",
    "class AngleSmoother:\n",
    "    def __init__(self, momentum=0.75):\n",
    "        self.momentum = momentum\n",
    "        self.value = None\n",
    "\n",
    "    def update(self, angle):\n",
    "        if self.value is None:\n",
    "            self.value = angle\n",
    "        else:\n",
    "            self.value = self.momentum * self.value + (1 - self.momentum) * angle\n",
    "        return self.value\n",
    "\n",
    "# 여러 얼굴에 대한 스무더 풀\n",
    "smoothers = {}\n",
    "\n",
    "def get_smoother(idx, momentum=0.8):\n",
    "    if idx not in smoothers:\n",
    "        smoothers[idx] = AngleSmoother(momentum=momentum)\n",
    "    return smoothers[idx]\n",
    "\n",
    "# 간단한 중복 체크용 헬퍼 (센터 거리)\n",
    "def is_near_existing(center, centers, thr):\n",
    "    for c in centers:\n",
    "        if np.linalg.norm(np.array(center) - np.array(c)) < thr:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# 실시간 웹캠 처리 + 녹화\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError('웹캠을 열 수 없습니다.')\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "recording = False\n",
    "writer = None\n",
    "save_path = None\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results_mesh = face_mesh.process(rgb)\n",
    "        results_det = face_det.process(rgb)\n",
    "\n",
    "        output = frame.copy()\n",
    "        used_centers = []\n",
    "        idx_counter = 0\n",
    "\n",
    "        if results_mesh.multi_face_landmarks:\n",
    "            for face_landmarks in results_mesh.multi_face_landmarks:\n",
    "                bbox = get_face_bbox(face_landmarks.landmark, frame.shape)\n",
    "                angle_raw = get_face_angle(face_landmarks.landmark, frame.shape)\n",
    "                angle = get_smoother(idx_counter).update(angle_raw)\n",
    "                skin_mask = build_skin_mask(face_landmarks.landmark, frame.shape)\n",
    "                skin_frame = blend_skin(output, skin_mask)\n",
    "                char_img = select_character(bbox, angle, frame.shape)\n",
    "                output = overlay_character(skin_frame, bbox, angle, char_img, alpha=0.95, scale=1.12)\n",
    "                used_centers.append((bbox[4], bbox[5]))\n",
    "                idx_counter += 1\n",
    "\n",
    "        if results_det.detections:\n",
    "            for det in results_det.detections:\n",
    "                bbox, angle_raw = bbox_angle_from_detection(det, frame.shape)\n",
    "                cx, cy = bbox[4], bbox[5]\n",
    "                thr = max(bbox[2], bbox[3]) * 0.35\n",
    "                if is_near_existing((cx, cy), used_centers, thr):\n",
    "                    continue\n",
    "                angle = get_smoother(idx_counter).update(angle_raw)\n",
    "                skin_mask = build_ellipse_mask(bbox, frame.shape)\n",
    "                skin_frame = blend_skin(output, skin_mask)\n",
    "                char_img = select_character(bbox, angle, frame.shape)\n",
    "                output = overlay_character(skin_frame, bbox, angle, char_img, alpha=0.95, scale=1.12)\n",
    "                used_centers.append((cx, cy))\n",
    "                idx_counter += 1\n",
    "\n",
    "        if recording and writer is not None:\n",
    "            writer.write(output)\n",
    "            cv2.putText(output, 'REC', (12, 32), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)\n",
    "\n",
    "        cv2.imshow('cartoon face overlay', output)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        if key == ord('r'):\n",
    "            if not recording:\n",
    "                ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                save_path = f'cartoon_capture_{ts}.mp4'\n",
    "                fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "                writer = cv2.VideoWriter(save_path, fourcc, fps, (output.shape[1], output.shape[0]))\n",
    "                recording = True\n",
    "                print(f'녹화 시작: {save_path}')\n",
    "            else:\n",
    "                recording = False\n",
    "                if writer is not None:\n",
    "                    writer.release()\n",
    "                    print(f'녹화 종료: {save_path}')\n",
    "                writer = None\n",
    "\n",
    "finally:\n",
    "    if writer is not None:\n",
    "        writer.release()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb74f5f",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart TD\n",
    "  A[Start/Init] --> B[Capture frame]\n",
    "  B --> C{FaceMesh ok?}\n",
    "  C -- yes --> D[Landmarks -> bbox/angle]\n",
    "  C -- no --> E[FaceDetection -> bbox/angle]\n",
    "  D --> F[Skin mask]\n",
    "  E --> F\n",
    "  F --> G[Select PNG by angle/size]\n",
    "  G --> H[Rotate/Scale/Blend]\n",
    "  H --> I[Display/Record]\n",
    "  I --> B\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labelme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
